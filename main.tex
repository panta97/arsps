

\documentclass[twocolumn]{article}

\usepackage[a4paper,bindingoffset=0in,%
            left=0.5in,right=0.5in,top=0.5in,bottom=1in,%
            footskip=.25in]{geometry}
            
\usepackage[backend=biber,sorting=none]{biblatex}
\addbibresource{refs.bib}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor=black
}


\title{Augmented Reality Sound Painting System}
\author{}
\date{}

\setlength{\columnsep}{0.5in}

\begin{document}
\maketitle

 \section*{Abstract}
 Body language is considered a way of non-verbal expression, where the human being had issued 
 their behavior and ideas since ancient times. Nowadays, this type of language plays an 
 important role in communication particularly in audiovisual media. Because of the increasing 
 technological advances and the easy access to them, a growing audiovisual culture is being 
 perceived in the digital media. This media content is mostly uploaded with art embedded in 
 it, which produces a compelling sensation by the people who consume it. New technologies such
 as virtual and augmented reality make a more artistic audiovisual content possible and offer 
 new ways of expressing art-content. As a result, the present research will pursue the 
 development of an augmented reality mobile application whose main feature will be 
 motion-capture and color-sound generation.


 \noindent\textbf{Keywords} Augmented Reality; Body Tracking; Mobile development; Apps; iOS; Sound Painting.
 
 
 \section{INTRODUCTION}
 The research begins with the need to have a digital medium that allows the general public to 
 perform a show of rhythms and colors portrayed in augmented reality, using body movements as 
 a means of entry, to develop and improve creativity. The objective of the project is to 
 implement an augmented reality mobile application that allows the general public to generate 
 3D drawings and rhythmic sounds using body movements as input.
 
 
 \section{STATE OF THE ART}
 Various research has been done regarding to body tracking, sound
 generation, augmented reality and mobile development for
 creating compelling applications. However, these technologies
 have been studied in isolation or at most using two of the
 mentioned technologies.
\bigskip


We have the need to use different technologies mainly augmented   reality which has 
drastically changed the way information is perceived, this has flourished thanks to the emerge 
of capable hardware and software \cite{UtilizingA}. According to \citeauthor{Anovelinte}, they 
conclude that augmented reality gives users a compelling and exciting effect, as it creates 
curiosity; In addition, it offers users the ability to interact with the physical and virtual
world at the same time. Therefore, with this approach, coaches and teachers have the 
opportunity to explore different physical objects to illustrate concepts using the same 
interface in the smartphone, without the need to change the overall graphics 
\cite{Anovelinte}. Another way to interact using AR can also be seen in \textcite{LeapMotion}
work , they developed an augmented reality environment using an Android device with Leap 
Motion (LM). This approach is similar in comparison to A brush device with visual and haptic 
feedback for virtual painting of 3D virtual objects in which users interact with a brush 
device \citeauthor{Abrushdevi}, this deviceâ€™s mechanisms provide haptic and visual feedback 
and are controlled by three cables that are simultaneously attached to one motor each 
\cite{Abrushdevi}. This AR painting experience can also be applied in makeup, 
\citeauthor{MakeupCrea} developed a novel interactive facial makeup system that aims to help 
users enhance makeup creativity \cite{MakeupCrea}. In this system, Kinect tracked the facial 
feature points and assigned them to the 3D face model. Another study focused in AR painting 
is the study of \citeauthor{Bare-Hande} Bare-Handed 3D Drawing in Augmented Reality, this study 
does not use any painting device but a head-mounted AR device, the mentioned study states 
that, to draw a line, the user performs the air touch gesture. The line is continuously drawn
at the position of the index cursor until the user ends the line by executing a second touch 
of air. The Tapline technique allows the user to draw a line by specifying control points 
\cite{Bare-Hande}. Similarly, this same head-mounted device is used in the research of Blowing 
in the wind: Increasing social presence with a virtual human via environmental airflow 
interaction in mixed reality by \citeauthor{Blowingint}; the research states that the 
head-mounted device gives the user a 3D virtual content experience as if it were spatially 
located in the physical environment \cite{Blowingint}.\citeauthor{ARPointer:} developed a system
that allow users to manipulate three-dimensional (3D) virtual objects in an AR environment, its 
device called AR Pointer uses a built-in Inertial Measurement Unit (IMU) sensor in a standard mobile
package to emit a virtual beam that is used to select objects with precision.\cite{ARPointer:}
\bigskip


Up to this point, the mentioned studies have focused on the use of AR only. In the case of 
body tracking all the reviewed studies have a similar approach which is to use a skeleton 
system. \citeauthor{3Dmotionca} proposes the following, a human motion capture system that is
inexpensive, portable, marker-less (non-invasive), requires no calibration equipment and 
performs at accuracy rates on par with the high-end advanced motion tracking systems 
\cite{3Dmotionca}. The study of \citeauthor{TowardsHum} states that, they developed an 
experiment where a minimized portable detection platform was specially designed to track 
human movement in a 3D setting. Their goal was to collect spatial and temporal information 
during the human movement process. \cite{TowardsHum}. \citeauthor{HumanMotio}, propose a new 
method of capturing human motion based on a cooperative structure of low-cost RGBD multiple 
cameras, to effectively avoid problems when tracking human movement through cameras 
low-cost depth with low-quality images\cite{HumanMotio}. \citeauthor{Humanactio}, developed an 
experiment where multilayer neural networks were used. It consists of many layers of interconnected 
neural units: it starts with an input layer to match the feature space followed by a non-linearity 
layer and ends with a classification layer to match the output space. It was experimentally proven 
that after recalculating the motion capture data (MoCap) to represent an invariant position, the 
classifier can directly use it to successfully recognize various types of actions\cite{Humanactio}.
\citeauthor{Real-TimeJ} developed a real-time, focusing tracking system for simultaneous tracking 
of handheld objects based on a single product depth sensor. The classification is based on a 
multilayer forest architecture with selection of viewpoints. A 3D articulated Gaussian blend 
alignment designed for handheld object tracking is utilized along with new analytical occlusion 
constraints and contact handling that enable successful tracking of challenging hand-object
interactions based on multiple proposals\cite{Real-TimeJ}. \citeauthor{Humanupper}, developed 
a virtual reality application which consists of a simple kinematic chain between the head and 
the hands. The parametric solution begins at the head and follows the chain to the hands. The 
IK solver is based on knowledge of arm length and body size (height), which must be 
calibrated\cite{Humanupper}. \citeauthor{Choreomorp}, developed a system which has the effect of 
seeing oneself, as a digital body in motion. Therefore, the Choreomorphy pipeline setup resembles 
a real-time inertial motion capture session. Regardless of where Choreomorphy is implemented, the 
environment should be a dark room with a space design of at least 12 square meters so that the user 
can move freely \cite{Choreomorp}.
\bigskip

\clearpage

\printbibliography

\end{document}
